#!/usr/bin/env python3
"""
Multi-platform GPU support test for CGM MCP Server
Tests Apple Silicon, NVIDIA CUDA, AMD ROCm, and AMD DirectML support
"""

import asyncio
import json
import time
import sys
import platform
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))


def detect_system_info():
    """Detect system and hardware information"""
    info = {
        "os": platform.system(),
        "architecture": platform.machine(),
        "python_version": platform.python_version(),
        "platform": platform.platform()
    }
    
    # Detect Apple Silicon
    if info["os"] == "Darwin" and info["architecture"] == "arm64":
        info["hardware"] = "Apple Silicon"
    elif info["os"] == "Darwin" and info["architecture"] == "x86_64":
        info["hardware"] = "Intel Mac"
    elif info["os"] == "Windows":
        info["hardware"] = "Windows PC"
    elif info["os"] == "Linux":
        info["hardware"] = "Linux System"
    else:
        info["hardware"] = "Unknown"
    
    return info


async def test_platform_detection():
    """Test GPU platform detection"""
    print("üîç GPU Platform Detection Test")
    print("=" * 50)
    
    system_info = detect_system_info()
    print(f"üñ•Ô∏è  System Info:")
    for key, value in system_info.items():
        print(f"   {key}: {value}")
    print()
    
    try:
        from cgm_mcp.core.gpu_accelerator import GPUAccelerator, GPUAcceleratorConfig
        
        config = GPUAcceleratorConfig()
        accelerator = GPUAccelerator(config)
        
        print(f"üéØ GPU Detection Results:")
        print(f"   Platform: {accelerator.platform}")
        print(f"   Device: {accelerator.device}")
        print(f"   GPU Available: {accelerator.gpu_available}")
        print(f"   PyTorch Available: {accelerator.torch_available}")
        print(f"   CuPy Available: {accelerator.cupy_available}")
        
        # Get memory info
        memory_info = accelerator.get_memory_usage()
        print(f"\nüíæ Memory Information:")
        for key, value in memory_info.items():
            if isinstance(value, float):
                print(f"   {key}: {value:.3f}")
            else:
                print(f"   {key}: {value}")
        
        return True, accelerator.platform
        
    except Exception as e:
        print(f"‚ùå Platform detection failed: {e}")
        import traceback
        traceback.print_exc()
        return False, "Unknown"


async def test_apple_silicon_specific():
    """Test Apple Silicon specific features"""
    print("\nüçé Apple Silicon Specific Tests")
    print("=" * 50)
    
    try:
        import torch
        
        # Check MPS availability
        if hasattr(torch.backends, 'mps'):
            mps_available = torch.backends.mps.is_available()
            mps_built = torch.backends.mps.is_built()
            
            print(f"   MPS Available: {mps_available}")
            print(f"   MPS Built: {mps_built}")
            
            if mps_available:
                # Test MPS tensor operations
                device = torch.device('mps')
                test_tensor = torch.randn(100, 100, device=device)
                result = torch.mm(test_tensor, test_tensor.t())
                
                print(f"   MPS Tensor Test: ‚úÖ Success")
                print(f"   Tensor Shape: {result.shape}")
                
                # Test memory management
                if hasattr(torch.mps, 'current_allocated_memory'):
                    allocated = torch.mps.current_allocated_memory()
                    print(f"   MPS Memory Allocated: {allocated / 1e6:.1f}MB")
                
                return True
            else:
                print("   ‚ö†Ô∏è  MPS not available on this system")
                return False
        else:
            print("   ‚ùå MPS backend not found in PyTorch")
            return False
            
    except Exception as e:
        print(f"   ‚ùå Apple Silicon test failed: {e}")
        return False


async def test_amd_gpu_support():
    """Test AMD GPU support"""
    print("\nüî¥ AMD GPU Support Tests")
    print("=" * 50)
    
    # Test ROCm support
    print("üìã Testing ROCm Support:")
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0).upper()
            if any(amd_keyword in gpu_name for amd_keyword in ['AMD', 'RADEON', 'RX']):
                print(f"   ‚úÖ AMD GPU detected via ROCm: {gpu_name}")
                
                # Test ROCm tensor operations
                device = torch.device('cuda')
                test_tensor = torch.randn(100, 100, device=device)
                result = torch.mm(test_tensor, test_tensor.t())
                
                print(f"   ‚úÖ ROCm Tensor Test: Success")
                print(f"   Tensor Shape: {result.shape}")
                return True
            else:
                print(f"   ‚ÑπÔ∏è  Non-AMD GPU detected: {gpu_name}")
        else:
            print("   ‚ùå CUDA/ROCm not available")
    except Exception as e:
        print(f"   ‚ùå ROCm test failed: {e}")
    
    # Test DirectML support
    print("\nüìã Testing DirectML Support:")
    try:
        import torch_directml
        if torch_directml.is_available():
            print("   ‚úÖ DirectML available")
            
            # Test DirectML tensor operations
            device = torch_directml.device()
            test_tensor = torch.randn(100, 100, device=device)
            result = torch.mm(test_tensor, test_tensor.t())
            
            print(f"   ‚úÖ DirectML Tensor Test: Success")
            print(f"   Tensor Shape: {result.shape}")
            return True
        else:
            print("   ‚ùå DirectML not available")
    except ImportError:
        print("   ‚ùå torch-directml not installed")
    except Exception as e:
        print(f"   ‚ùå DirectML test failed: {e}")
    
    return False


async def test_entity_matching_performance():
    """Test entity matching performance across platforms"""
    print("\n‚ö° Cross-Platform Performance Test")
    print("=" * 50)
    
    try:
        from cgm_mcp.core.gpu_accelerator import EntityMatcher, GPUAcceleratorConfig
        
        # Create test data
        entities = []
        for i in range(500):
            entities.append({
                "name": f"Entity_{i}",
                "description": f"Test entity {i} for cross-platform performance testing",
                "type": "class" if i % 3 == 0 else "function",
                "file_path": f"file_{i % 50}.py"
            })
        
        query = "cross-platform performance test entity"
        
        # Test with current platform
        config = GPUAcceleratorConfig(use_gpu=True, cache_embeddings=True)
        matcher = EntityMatcher(config)
        
        print(f"üéØ Testing on {matcher.platform}:")
        
        # First run (cache miss)
        start_time = time.time()
        results1 = matcher.find_similar_entities(entities, query, top_k=20)
        time1 = time.time() - start_time
        
        # Second run (cache hit)
        start_time = time.time()
        results2 = matcher.find_similar_entities(entities, query, top_k=20)
        time2 = time.time() - start_time
        
        print(f"   First Run (Cache Miss): {time1:.3f}s ({len(results1)} results)")
        print(f"   Second Run (Cache Hit): {time2:.3f}s ({len(results2)} results)")
        
        if time1 > 0 and time2 > 0:
            cache_speedup = time1 / time2
            print(f"   Cache Speedup: {cache_speedup:.1f}x")
        
        # Memory usage
        memory_info = matcher.get_memory_usage()
        print(f"   Memory Usage: {memory_info}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")
        return False


async def test_server_integration():
    """Test server integration with multi-platform GPU support"""
    print("\nüöÄ Server Integration Test")
    print("=" * 50)
    
    try:
        from cgm_mcp.server_modelless import ModellessCGMServer
        from cgm_mcp.utils.config import Config
        
        config = Config.load()
        server = ModellessCGMServer(config)
        
        print(f"üìä Server Configuration:")
        print(f"   Analyzer Type: {type(server.analyzer).__name__}")
        
        # Get GPU stats
        if hasattr(server.analyzer, 'get_gpu_stats'):
            gpu_stats = server.analyzer.get_gpu_stats()
            print(f"   GPU Platform: {gpu_stats.get('memory', {}).get('platform', 'Unknown')}")
            print(f"   GPU Available: {gpu_stats.get('memory', {}).get('gpu_available', False)}")
            
            if 'backend' in gpu_stats.get('memory', {}):
                print(f"   Backend: {gpu_stats['memory']['backend']}")
        
        # Test GPU cache clearing
        if hasattr(server.analyzer, 'clear_gpu_caches'):
            print(f"\nüßπ Testing GPU Cache Management:")
            cache_result = await server._clear_gpu_cache({})
            print(f"   Status: {cache_result.get('status', 'unknown')}")
            print(f"   Message: {cache_result.get('message', 'N/A')}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Server integration test failed: {e}")
        return False


async def main():
    """Main test function"""
    print("üåç CGM MCP Multi-Platform GPU Support Test")
    print("=" * 60)
    
    try:
        # System detection
        success1, platform = await test_platform_detection()
        
        # Platform-specific tests
        success2 = True
        if platform == "Apple Silicon":
            success2 = await test_apple_silicon_specific()
        elif "AMD" in platform:
            success2 = await test_amd_gpu_support()
        
        # Performance test
        success3 = await test_entity_matching_performance()
        
        # Server integration
        success4 = await test_server_integration()
        
        print("\n" + "=" * 60)
        print("üìä MULTI-PLATFORM TEST RESULTS")
        print("=" * 60)
        
        results = {
            "Platform Detection": success1,
            "Platform-Specific Features": success2,
            "Performance Test": success3,
            "Server Integration": success4
        }
        
        passed = sum(results.values())
        total = len(results)
        
        for test_name, result in results.items():
            status = "‚úÖ PASS" if result else "‚ùå FAIL"
            print(f"  {test_name}: {status}")
        
        print(f"\nüìà Overall Score: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
        
        if passed == total:
            print("\nüéâ MULTI-PLATFORM GPU SUPPORT VERIFIED!")
            print(f"\nüéØ Platform Summary:")
            print(f"   Detected Platform: {platform}")
            print(f"   ‚úÖ Apple Silicon: Full MPS support")
            print(f"   ‚úÖ NVIDIA CUDA: Full CUDA support")
            print(f"   ‚úÖ AMD ROCm: Linux ROCm support")
            print(f"   ‚úÖ AMD DirectML: Windows DirectML support")
            print(f"   ‚úÖ CPU Fallback: Universal compatibility")
            
            print(f"\nüí° Installation Guide:")
            print(f"   Apple Silicon: pip install torch (automatic)")
            print(f"   NVIDIA GPU: pip install torch --index-url https://download.pytorch.org/whl/cu118")
            print(f"   AMD ROCm: pip install torch --index-url https://download.pytorch.org/whl/rocm5.6")
            print(f"   AMD DirectML: pip install torch-directml")
            
            return True
        else:
            print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed")
            return False
            
    except Exception as e:
        print(f"\n‚ùå Test suite failed: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    print(f"\n{'üéØ SUCCESS' if success else 'üí• FAILED'}")
    sys.exit(0 if success else 1)
