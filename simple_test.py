#!/usr/bin/env python3
"""
Simple test for optimized CGM MCP server
"""

import asyncio
import json
import time
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from cgm_mcp.server_modelless import ModellessCGMServer
from cgm_mcp.utils.config import Config


async def test_optimized_server():
    """Test the optimized server functionality"""
    print("ğŸš€ Testing Optimized CGM MCP Server")
    print("=" * 50)
    
    try:
        # Initialize server
        config = Config.load()
        server = ModellessCGMServer(config)
        
        # Test repository path (use current directory)
        repo_path = str(Path.cwd())
        
        print(f"ğŸ“ Testing with repository: {repo_path}")
        print()
        
        # Test 1: Repository analysis
        print("ğŸ” Test 1: Repository Analysis")
        start_time = time.time()
        
        result1 = await server._analyze_repository({
            "repository_path": repo_path,
            "query": "server optimization performance",
            "analysis_scope": "focused",
            "max_files": 3
        })
        
        end_time = time.time()
        print(f"â±ï¸  First analysis took: {end_time - start_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result1.get('status', 'unknown')}")
        print(f"ğŸ“„ Files analyzed: {result1.get('file_count', 0)}")
        print(f"ğŸ”— Entities found: {result1.get('entity_count', 0)}")
        print()
        
        # Test 2: Cached analysis (should be faster)
        print("ğŸ” Test 2: Cached Repository Analysis")
        start_time = time.time()
        
        result2 = await server._analyze_repository({
            "repository_path": repo_path,
            "query": "server optimization performance",
            "analysis_scope": "focused",
            "max_files": 3
        })
        
        end_time = time.time()
        print(f"â±ï¸  Cached analysis took: {end_time - start_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result2.get('status', 'unknown')}")
        print()
        
        # Test 3: File content analysis
        print("ğŸ” Test 3: File Content Analysis")
        start_time = time.time()
        
        result3 = await server._get_file_content({
            "repository_path": repo_path,
            "file_paths": ["src/cgm_mcp/server_modelless.py"]
        })
        
        end_time = time.time()
        print(f"â±ï¸  File analysis took: {end_time - start_time:.2f} seconds")
        print(f"ğŸ“Š Status: {result3.get('status', 'unknown')}")
        print(f"ğŸ“„ Files processed: {result3.get('file_count', 0)}")
        print()
        
        # Test 4: Performance metrics
        print("ğŸ“Š Performance Metrics")
        try:
            perf_info = server._get_memory_usage()
            
            print(f"ğŸ’¾ Memory Usage: {perf_info['rss_mb']:.1f} MB")
            print(f"ğŸ“ˆ Memory Percent: {perf_info['percent']:.1f}%")
            print()
            
        except Exception as e:
            print(f"âŒ Error getting performance metrics: {e}")
            print()
        
        # Test 5: Cache information
        print("ğŸ—„ï¸  Cache Information")
        try:
            print(f"ğŸ“¦ Analysis Cache: {len(server.analysis_cache)}/{server.analysis_cache.maxsize}")
            print(f"ğŸ“„ File Cache: {len(server.file_cache)}/{server.file_cache.maxsize}")
            print(f"ğŸŒ³ AST Cache: {len(server.ast_cache)}/{server.ast_cache.maxsize}")
            print(f"ğŸ¯ Cache Hits: {server.cache_stats['hits']}")
            print(f"âŒ Cache Misses: {server.cache_stats['misses']}")
            print(f"ğŸ“ File Cache Hits: {server.cache_stats['file_hits']}")
            print(f"ğŸ“ File Cache Misses: {server.cache_stats['file_misses']}")
            
            # Calculate hit rates
            total_requests = server.cache_stats['hits'] + server.cache_stats['misses']
            if total_requests > 0:
                hit_rate = server.cache_stats['hits'] / total_requests
                print(f"ğŸ¯ Overall Cache Hit Rate: {hit_rate:.2%}")
            
            file_requests = server.cache_stats['file_hits'] + server.cache_stats['file_misses']
            if file_requests > 0:
                file_hit_rate = server.cache_stats['file_hits'] / file_requests
                print(f"ğŸ“ File Cache Hit Rate: {file_hit_rate:.2%}")
            
            print()
            
        except Exception as e:
            print(f"âŒ Error getting cache info: {e}")
            print()
        
        print("âœ… Optimization test completed successfully!")
        print("=" * 50)
        
        # Summary of optimizations
        print("\nğŸ¯ Optimizations Implemented:")
        print("  âœ… TTL Cache (1 hour expiration)")
        print("  âœ… LRU File Cache (500 entries)")
        print("  âœ… AST Parsing Cache (200 entries)")
        print("  âœ… Memory Usage Monitoring")
        print("  âœ… Automatic Cache Cleanup")
        print("  âœ… Concurrent File Processing")
        print("  âœ… Enhanced Cache Key Generation")
        print("  âœ… Performance Statistics")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_optimized_server())
    if success:
        print("\nğŸ‰ All tests passed! The optimized server is working correctly.")
    else:
        print("\nğŸ’¥ Tests failed. Please check the error messages above.")
        sys.exit(1)
