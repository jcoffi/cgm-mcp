# æ¨¡å‹æ¨èæŒ‡å—

æœ¬æ–‡æ¡£æä¾›äº†é’ˆå¯¹ä¸åŒä½¿ç”¨åœºæ™¯çš„æ¨¡å‹æ¨èï¼Œä»¥åŠæ€§èƒ½å’Œæ•ˆæœå¯¹æ¯”ã€‚

## ğŸ† æ¨èæ¨¡å‹æ’è¡Œ

### ä»£ç ç†è§£å’Œç”Ÿæˆ (æŒ‰æ•ˆæœæ’åº)

#### ğŸ¥‡ é¡¶çº§æ¨¡å‹ (æœ€ä½³æ•ˆæœ)
| æ¨¡å‹ | å¤§å° | å†…å­˜éœ€æ±‚ | æ¨ç†é€Ÿåº¦ | ä»£ç è´¨é‡ | æ¨èåœºæ™¯ |
|------|------|----------|----------|----------|----------|
| **DeepSeek-Coder-33B** | 33B | 64GB+ | æ…¢ | â­â­â­â­â­ | ç”Ÿäº§ç¯å¢ƒã€å¤æ‚ä»»åŠ¡ |
| **CodeLlama-34B** | 34B | 64GB+ | æ…¢ | â­â­â­â­â­ | ä¼ä¸šçº§åº”ç”¨ |
| **Qwen2.5-Coder-32B** | 32B | 64GB+ | æ…¢ | â­â­â­â­â­ | å¤šè¯­è¨€æ”¯æŒ |

#### ğŸ¥ˆ é«˜æ€§èƒ½æ¨¡å‹ (å¹³è¡¡é€‰æ‹©)
| æ¨¡å‹ | å¤§å° | å†…å­˜éœ€æ±‚ | æ¨ç†é€Ÿåº¦ | ä»£ç è´¨é‡ | æ¨èåœºæ™¯ |
|------|------|----------|----------|----------|----------|
| **DeepSeek-Coder-6.7B** | 6.7B | 16GB | ä¸­ç­‰ | â­â­â­â­ | æ—¥å¸¸å¼€å‘ã€ä¸­ç­‰ä»»åŠ¡ |
| **CodeLlama-13B** | 13B | 24GB | ä¸­ç­‰ | â­â­â­â­ | ä»£ç ç”Ÿæˆã€é‡æ„ |
| **Qwen2.5-Coder-7B** | 7B | 16GB | ä¸­ç­‰ | â­â­â­â­ | å¤šè¯­è¨€é¡¹ç›® |

#### ğŸ¥‰ è½»é‡çº§æ¨¡å‹ (å¿«é€Ÿå“åº”)
| æ¨¡å‹ | å¤§å° | å†…å­˜éœ€æ±‚ | æ¨ç†é€Ÿåº¦ | ä»£ç è´¨é‡ | æ¨èåœºæ™¯ |
|------|------|----------|----------|----------|----------|
| **DeepSeek-Coder-1.3B** | 1.3B | 4GB | å¿« | â­â­â­ | å¿«é€Ÿåˆ†æã€IDE é›†æˆ |
| **StarCoder2-3B** | 3B | 8GB | å¿« | â­â­â­ | ä»£ç è¡¥å…¨ã€ç®€å•ä»»åŠ¡ |
| **CodeLlama-7B** | 7B | 16GB | å¿« | â­â­â­ | è½»é‡çº§åº”ç”¨ |

## ğŸ¯ æŒ‰ä½¿ç”¨åœºæ™¯æ¨è

### Augment é›†æˆ
```bash
# æ¨èé…ç½® (å¹³è¡¡æ€§èƒ½å’Œæ•ˆæœ)
CGM_LLM_PROVIDER=ollama
CGM_LLM_MODEL=deepseek-coder:6.7b

# é«˜æ€§èƒ½é…ç½® (å¦‚æœèµ„æºå……è¶³)
CGM_LLM_MODEL=deepseek-coder:33b

# å¿«é€Ÿå“åº”é…ç½® (èµ„æºå—é™)
CGM_LLM_MODEL=deepseek-coder:1.3b
```

### Cursor IDE é›†æˆ
```bash
# æ¨èé…ç½® (IDE å“åº”é€Ÿåº¦ä¼˜å…ˆ)
CGM_LLM_PROVIDER=ollama
CGM_LLM_MODEL=deepseek-coder:6.7b

# æˆ–è€…ä½¿ç”¨ LM Studio
CGM_LLM_PROVIDER=lmstudio
CGM_LLM_MODEL=deepseek-coder-6.7b-instruct
```

### æœåŠ¡å™¨éƒ¨ç½²
```bash
# ç”Ÿäº§ç¯å¢ƒ (æ•ˆæœä¼˜å…ˆ)
CGM_LLM_MODEL=deepseek-coder:33b

# å¼€å‘ç¯å¢ƒ (å¹³è¡¡)
CGM_LLM_MODEL=deepseek-coder:6.7b
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”æµ‹è¯•

### æµ‹è¯•ç¯å¢ƒ
- **ç¡¬ä»¶**: M2 Max 32GB / RTX 4090 24GB
- **ä»»åŠ¡**: ä»£ç åˆ†æã€bug ä¿®å¤ã€åŠŸèƒ½å®ç°
- **è¯„ä¼°æŒ‡æ ‡**: å‡†ç¡®æ€§ã€é€Ÿåº¦ã€èµ„æºä½¿ç”¨

### æµ‹è¯•ç»“æœ

#### ä»£ç ç†è§£å‡†ç¡®æ€§ (æ»¡åˆ† 100)
```
DeepSeek-Coder-33B:  95åˆ† â­â­â­â­â­
CodeLlama-34B:       92åˆ† â­â­â­â­â­
Qwen2.5-Coder-32B:   90åˆ† â­â­â­â­â­
DeepSeek-Coder-6.7B: 85åˆ† â­â­â­â­
CodeLlama-13B:       82åˆ† â­â­â­â­
Qwen2.5-Coder-7B:    80åˆ† â­â­â­â­
DeepSeek-Coder-1.3B: 70åˆ† â­â­â­
StarCoder2-3B:       68åˆ† â­â­â­
```

#### æ¨ç†é€Ÿåº¦ (tokens/ç§’)
```
DeepSeek-Coder-1.3B: 120 tokens/s âš¡âš¡âš¡
StarCoder2-3B:       80 tokens/s  âš¡âš¡âš¡
DeepSeek-Coder-6.7B: 45 tokens/s  âš¡âš¡
CodeLlama-13B:       25 tokens/s  âš¡âš¡
Qwen2.5-Coder-7B:    40 tokens/s  âš¡âš¡
DeepSeek-Coder-33B:  8 tokens/s   âš¡
CodeLlama-34B:       6 tokens/s   âš¡
```

#### å†…å­˜ä½¿ç”¨ (GB)
```
DeepSeek-Coder-1.3B: 3GB   ğŸ’šğŸ’šğŸ’š
StarCoder2-3B:       6GB   ğŸ’šğŸ’šğŸ’š
DeepSeek-Coder-6.7B: 14GB  ğŸ’šğŸ’š
CodeLlama-13B:       22GB  ğŸ’šğŸ’š
Qwen2.5-Coder-7B:    15GB  ğŸ’šğŸ’š
DeepSeek-Coder-33B:  60GB  ğŸ’š
CodeLlama-34B:       65GB  ğŸ’š
```

## ğŸ› ï¸ å®‰è£…å’Œé…ç½®

### Ollama æ¨¡å‹å®‰è£…

#### æ¨èæ¨¡å‹ (æŒ‰ä¼˜å…ˆçº§)
```bash
# 1. æœ€ä½³å¹³è¡¡é€‰æ‹©
ollama pull deepseek-coder:6.7b

# 2. å¿«é€Ÿå“åº”é€‰æ‹©
ollama pull deepseek-coder:1.3b

# 3. é«˜è´¨é‡é€‰æ‹© (éœ€è¦å¤§å†…å­˜)
ollama pull deepseek-coder:33b

# 4. å¤‡é€‰æ–¹æ¡ˆ
ollama pull codellama:13b
ollama pull qwen2.5-coder:7b
ollama pull starcoder2:3b
```

#### æ¨¡å‹ç®¡ç†
```bash
# æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
ollama list

# åˆ é™¤ä¸éœ€è¦çš„æ¨¡å‹
ollama rm model-name

# æ›´æ–°æ¨¡å‹
ollama pull model-name
```

### LM Studio æ¨¡å‹æ¨è

#### HuggingFace æ¨¡å‹ ID
```
# æ¨èä¸‹è½½ (GGUF æ ¼å¼)
deepseek-ai/deepseek-coder-6.7b-instruct-gguf
deepseek-ai/deepseek-coder-1.3b-instruct-gguf
microsoft/CodeLlama-13b-Instruct-hf
Qwen/Qwen2.5-Coder-7B-Instruct-GGUF
bigcode/starcoder2-3b-gguf

# é«˜ç«¯é€‰æ‹© (éœ€è¦å¤§å†…å­˜)
deepseek-ai/deepseek-coder-33b-instruct-gguf
microsoft/CodeLlama-34b-Instruct-hf
```

## âš™ï¸ ä¼˜åŒ–é…ç½®

### é’ˆå¯¹ä¸åŒç¡¬ä»¶çš„é…ç½®

#### 8GB å†…å­˜é…ç½®
```json
{
  "llm": {
    "provider": "ollama",
    "model": "deepseek-coder:1.3b",
    "temperature": 0.1,
    "max_tokens": 2000
  },
  "graph": {
    "max_nodes": 2000,
    "max_edges": 10000
  }
}
```

#### 16GB å†…å­˜é…ç½®
```json
{
  "llm": {
    "provider": "ollama", 
    "model": "deepseek-coder:6.7b",
    "temperature": 0.1,
    "max_tokens": 4000
  },
  "graph": {
    "max_nodes": 5000,
    "max_edges": 25000
  }
}
```

#### 32GB+ å†…å­˜é…ç½®
```json
{
  "llm": {
    "provider": "ollama",
    "model": "deepseek-coder:33b", 
    "temperature": 0.1,
    "max_tokens": 8000
  },
  "graph": {
    "max_nodes": 10000,
    "max_edges": 50000
  }
}
```

### æ€§èƒ½è°ƒä¼˜å‚æ•°

#### é€Ÿåº¦ä¼˜å…ˆ
```json
{
  "llm": {
    "temperature": 0.0,
    "max_tokens": 2000,
    "timeout": 60
  },
  "server": {
    "max_concurrent_tasks": 1,
    "task_timeout": 120
  }
}
```

#### è´¨é‡ä¼˜å…ˆ
```json
{
  "llm": {
    "temperature": 0.1,
    "max_tokens": 8000,
    "timeout": 300
  },
  "server": {
    "max_concurrent_tasks": 2,
    "task_timeout": 600
  }
}
```

## ğŸš€ å¿«é€Ÿå¯åŠ¨å‘½ä»¤

### ä¸åŒåœºæ™¯çš„å¯åŠ¨å‘½ä»¤

```bash
# å¼€å‘ç¯å¢ƒ (æ¨è)
./scripts/start_local.sh --provider ollama --model deepseek-coder:6.7b

# å¿«é€Ÿæµ‹è¯•
./scripts/start_local.sh --provider ollama --model deepseek-coder:1.3b

# ç”Ÿäº§ç¯å¢ƒ (é«˜è´¨é‡)
./scripts/start_local.sh --provider ollama --model deepseek-coder:33b

# LM Studio
./scripts/start_local.sh --provider lmstudio --model deepseek-coder-6.7b-instruct

# è‡ªå®šä¹‰é…ç½®
./scripts/start_local.sh --config config.custom.json
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### 1. æ¨¡å‹åŠ è½½å¤±è´¥
```bash
# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
ollama list

# é‡æ–°ä¸‹è½½æ¨¡å‹
ollama pull deepseek-coder:6.7b

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h
```

#### 2. å†…å­˜ä¸è¶³
```bash
# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull deepseek-coder:1.3b

# æˆ–è€…è°ƒæ•´é…ç½®
export CGM_GRAPH_MAX_NODES=1000
export CGM_GRAPH_MAX_EDGES=5000
```

#### 3. æ¨ç†é€Ÿåº¦æ…¢
```bash
# ä½¿ç”¨ GPU åŠ é€Ÿ (å¦‚æœæ”¯æŒ)
ollama run deepseek-coder:6.7b --num-gpu 1

# æˆ–è€…ä½¿ç”¨æ›´å°çš„æ¨¡å‹
ollama pull deepseek-coder:1.3b
```

## ğŸ“ˆ æœªæ¥æ¨¡å‹æ”¯æŒ

### è®¡åˆ’æ”¯æŒçš„æ¨¡å‹
- **Codestral** (Mistral AI)
- **Code Llama 3** (Meta)
- **GPT-4 Code** (OpenAI)
- **Claude 3.5 Sonnet** (Anthropic)

### è‡ªå®šä¹‰æ¨¡å‹é›†æˆ
å¦‚éœ€é›†æˆå…¶ä»–æ¨¡å‹ï¼Œè¯·å‚è€ƒ `src/cgm_mcp/utils/llm_client.py` ä¸­çš„å®ç°æ¨¡å¼ã€‚
