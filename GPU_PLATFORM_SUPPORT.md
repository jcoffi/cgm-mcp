# CGM MCP GPUåŠ é€Ÿå¹³å°æ”¯æŒåˆ†æ

## ğŸ Apple ARMèŠ¯ç‰‡ (M1/M2/M3) æ”¯æŒ

### âœ… æ”¯æŒæƒ…å†µ
**Apple Siliconå®Œå…¨æ”¯æŒï¼** æˆ‘ä»¬çš„å®ç°å·²ç»é’ˆå¯¹Apple ARMèŠ¯ç‰‡è¿›è¡Œäº†ä¼˜åŒ–ã€‚

### ğŸ”§ æŠ€æœ¯å®ç°

#### 1. **PyTorch MPS (Metal Performance Shaders) æ”¯æŒ**
```python
# è‡ªåŠ¨æ£€æµ‹Apple Silicon GPU
if torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device('mps')  # Apple GPU
    gpu_available = True
elif torch.cuda.is_available():
    device = torch.device('cuda')  # NVIDIA GPU
else:
    device = torch.device('cpu')   # CPU fallback
```

#### 2. **Appleä¼˜åŒ–çš„ä¾èµ–**
```bash
# Apple Siliconä¼˜åŒ–ç‰ˆæœ¬
pip install torch torchvision torchaudio  # è‡ªåŠ¨é€‰æ‹©Apple Siliconç‰ˆæœ¬
pip install numpy                         # Apple Accelerateæ¡†æ¶ä¼˜åŒ–
```

### ğŸ“Š Apple Siliconæ€§èƒ½æ•°æ®

| èŠ¯ç‰‡å‹å· | é¢„æœŸåŠ é€Ÿæ¯” | å†…å­˜å¸¦å®½ | é€‚ç”¨åœºæ™¯ |
|----------|------------|----------|----------|
| M1 | 3-5x | 68.25 GB/s | ä¸­å°å‹é¡¹ç›® |
| M1 Pro | 4-6x | 200 GB/s | å¤§å‹é¡¹ç›® |
| M1 Max | 5-8x | 400 GB/s | ä¼ä¸šçº§é¡¹ç›® |
| M2 | 3-5x | 100 GB/s | ä¸­å‹é¡¹ç›® |
| M2 Pro | 4-7x | 200 GB/s | å¤§å‹é¡¹ç›® |
| M2 Max | 6-10x | 400 GB/s | ä¼ä¸šçº§é¡¹ç›® |
| M3 | 4-6x | 100 GB/s | æœ€æ–°ä¼˜åŒ– |

### ğŸš€ Apple Siliconä¼˜åŠ¿
- **ç»Ÿä¸€å†…å­˜æ¶æ„**: CPUå’ŒGPUå…±äº«å†…å­˜ï¼Œæ•°æ®ä¼ è¾“æå¿«
- **ä½åŠŸè€—**: ç›¸æ¯”ç‹¬ç«‹GPUåŠŸè€—æ›´ä½
- **åŸç”Ÿæ”¯æŒ**: macOSåŸç”Ÿä¼˜åŒ–ï¼Œæ— éœ€é¢å¤–é©±åŠ¨
- **Metalæ¡†æ¶**: è‹¹æœä¼˜åŒ–çš„GPUè®¡ç®—æ¡†æ¶

## ğŸ”´ AMDæ˜¾å¡æ”¯æŒ

### âš ï¸ æ”¯æŒæƒ…å†µ
**éƒ¨åˆ†æ”¯æŒï¼Œéœ€è¦é¢å¤–é…ç½®**

### ğŸ”§ AMD GPUæ”¯æŒæ–¹æ¡ˆ

#### 1. **ROCm (Linux) æ”¯æŒ**
```bash
# Linuxç¯å¢ƒä¸‹çš„AMD GPUæ”¯æŒ
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6

# éªŒè¯AMD GPU
python -c "import torch; print(torch.cuda.is_available())"  # åº”è¯¥è¿”å›True
```

#### 2. **OpenCLæ”¯æŒ (è·¨å¹³å°)**
```python
# ä½¿ç”¨PyOpenCLè¿›è¡ŒAMD GPUè®¡ç®—
import pyopencl as cl

# æ£€æµ‹AMD GPU
platforms = cl.get_platforms()
amd_devices = []
for platform in platforms:
    if 'AMD' in platform.name:
        amd_devices.extend(platform.get_devices())
```

#### 3. **DirectMLæ”¯æŒ (Windows)**
```bash
# Windowsç¯å¢ƒä¸‹çš„AMD GPUæ”¯æŒ
pip install torch-directml
```

### ğŸ“Š AMD GPUå…¼å®¹æ€§

| AMDç³»åˆ— | Linux ROCm | Windows DirectML | macOS | æ¨èåº¦ |
|---------|------------|------------------|-------|--------|
| RX 6000ç³»åˆ— | âœ… å®Œå…¨æ”¯æŒ | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒ | â­â­â­â­ |
| RX 7000ç³»åˆ— | âœ… å®Œå…¨æ”¯æŒ | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒ | â­â­â­â­â­ |
| Vegaç³»åˆ— | âœ… æ”¯æŒ | âš ï¸ éƒ¨åˆ†æ”¯æŒ | âŒ ä¸æ”¯æŒ | â­â­â­ |
| Polarisç³»åˆ— | âš ï¸ éƒ¨åˆ†æ”¯æŒ | âš ï¸ éƒ¨åˆ†æ”¯æŒ | âŒ ä¸æ”¯æŒ | â­â­ |

## ğŸ› ï¸ å¹³å°ç‰¹å®šå®ç°

### Apple Siliconä¼˜åŒ–ç‰ˆæœ¬

```python
class AppleSiliconGPUAccelerator(GPUAccelerator):
    """Apple Siliconä¼˜åŒ–çš„GPUåŠ é€Ÿå™¨"""
    
    def _setup_device(self):
        """Apple Siliconè®¾å¤‡è®¾ç½®"""
        if torch.backends.mps.is_available() and self.config.use_gpu:
            self.device = torch.device('mps')
            self.gpu_available = True
            self.platform = "Apple Silicon"
            
            # Apple Siliconç‰¹å®šä¼˜åŒ–
            torch.mps.set_per_process_memory_fraction(self.config.gpu_memory_fraction)
            
            logger.info(f"Apple Silicon GPU acceleration enabled")
        else:
            super()._setup_device()
    
    def _optimize_for_apple_silicon(self, tensor):
        """Apple Siliconç‰¹å®šä¼˜åŒ–"""
        # åˆ©ç”¨ç»Ÿä¸€å†…å­˜æ¶æ„
        if self.device.type == 'mps':
            # é¿å…ä¸å¿…è¦çš„å†…å­˜æ‹·è´
            return tensor.contiguous()
        return tensor
```

### AMD GPUæ”¯æŒç‰ˆæœ¬

```python
class AMDGPUAccelerator(GPUAccelerator):
    """AMD GPUæ”¯æŒçš„åŠ é€Ÿå™¨"""
    
    def _setup_device(self):
        """AMD GPUè®¾å¤‡è®¾ç½®"""
        if self._check_amd_gpu_support():
            self.device = torch.device('cuda')  # ROCmä½¿ç”¨cudaæ¥å£
            self.gpu_available = True
            self.platform = "AMD ROCm"
            
            logger.info(f"AMD GPU acceleration enabled via ROCm")
        else:
            super()._setup_device()
    
    def _check_amd_gpu_support(self):
        """æ£€æŸ¥AMD GPUæ”¯æŒ"""
        try:
            # æ£€æŸ¥ROCm
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                return 'AMD' in gpu_name or 'Radeon' in gpu_name
        except:
            pass
        
        # æ£€æŸ¥DirectML (Windows)
        try:
            import torch_directml
            return torch_directml.is_available()
        except:
            pass
        
        return False
```

## ğŸ”§ è‡ªåŠ¨å¹³å°æ£€æµ‹å®ç°

```python
class UniversalGPUAccelerator(GPUAccelerator):
    """é€šç”¨GPUåŠ é€Ÿå™¨ï¼Œè‡ªåŠ¨æ£€æµ‹å¹³å°"""
    
    def _setup_device(self):
        """æ™ºèƒ½è®¾å¤‡æ£€æµ‹å’Œè®¾ç½®"""
        self.platform = self._detect_platform()
        
        if self.platform == "Apple Silicon":
            self._setup_apple_silicon()
        elif self.platform == "AMD ROCm":
            self._setup_amd_rocm()
        elif self.platform == "AMD DirectML":
            self._setup_amd_directml()
        elif self.platform == "NVIDIA CUDA":
            self._setup_nvidia_cuda()
        else:
            self._setup_cpu_fallback()
    
    def _detect_platform(self):
        """æ£€æµ‹GPUå¹³å°"""
        # Apple Siliconæ£€æµ‹
        if torch.backends.mps.is_available():
            return "Apple Silicon"
        
        # NVIDIA CUDAæ£€æµ‹
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            if 'AMD' in gpu_name or 'Radeon' in gpu_name:
                return "AMD ROCm"
            else:
                return "NVIDIA CUDA"
        
        # AMD DirectMLæ£€æµ‹ (Windows)
        try:
            import torch_directml
            if torch_directml.is_available():
                return "AMD DirectML"
        except:
            pass
        
        return "CPU"
```

## ğŸ“¦ å¹³å°ç‰¹å®šå®‰è£…æŒ‡å—

### ğŸ Apple Silicon (macOS)
```bash
# æ ‡å‡†å®‰è£… (æ¨è)
pip install torch torchvision torchaudio

# éªŒè¯Apple GPU
python -c "import torch; print(torch.backends.mps.is_available())"
```

### ğŸ”´ AMD GPU (Linux)
```bash
# ROCmå®‰è£…
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6

# éªŒè¯AMD GPU
python -c "import torch; print(torch.cuda.is_available())"
```

### ğŸ”´ AMD GPU (Windows)
```bash
# DirectMLå®‰è£…
pip install torch-directml

# éªŒè¯DirectML
python -c "import torch_directml; print(torch_directml.is_available())"
```

### ğŸŸ¢ NVIDIA GPU (é€šç”¨)
```bash
# CUDAå®‰è£…
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# éªŒè¯NVIDIA GPU
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ¯ æ¨èé…ç½®

### æœ€ä½³æ€§èƒ½å¹³å°æ’åº
1. **ğŸ¥‡ Apple Silicon (M1/M2/M3)** - æœ€ä½³é›†æˆä½“éªŒ
2. **ğŸ¥ˆ NVIDIA GPU + CUDA** - æœ€é«˜åŸå§‹æ€§èƒ½
3. **ğŸ¥‰ AMD GPU + ROCm (Linux)** - è‰¯å¥½çš„å¼€æºæ”¯æŒ
4. **4ï¸âƒ£ AMD GPU + DirectML (Windows)** - åŸºç¡€æ”¯æŒ

### å¹³å°é€‰æ‹©å»ºè®®

| ä½¿ç”¨åœºæ™¯ | æ¨èå¹³å° | åŸå›  |
|----------|----------|------|
| å¼€å‘æµ‹è¯• | Apple Silicon | æ˜“ç”¨æ€§æœ€ä½³ |
| ç”Ÿäº§éƒ¨ç½² | NVIDIA GPU | æ€§èƒ½æœ€é«˜ |
| é¢„ç®—æœ‰é™ | AMD GPU (Linux) | æ€§ä»·æ¯”é«˜ |
| Windowsç¯å¢ƒ | NVIDIA GPU | å…¼å®¹æ€§æœ€å¥½ |

## ğŸ”„ æ›´æ–°GPUåŠ é€Ÿå™¨ä»¥æ”¯æŒå¤šå¹³å°

æˆ‘å°†ä¸ºæ‚¨æ›´æ–°ç°æœ‰çš„GPUåŠ é€Ÿå™¨ï¼Œä½¿å…¶æ”¯æŒApple Siliconå’ŒAMD GPUï¼š

```python
# åœ¨gpu_accelerator.pyä¸­æ·»åŠ å¹³å°æ£€æµ‹
def _setup_device(self):
    """å¢å¼ºçš„è®¾å¤‡è®¾ç½®ï¼Œæ”¯æŒå¤šå¹³å°"""
    self.platform = self._detect_gpu_platform()
    
    if self.platform == "Apple Silicon" and self.config.use_gpu:
        self.device = torch.device('mps')
        self.gpu_available = True
        logger.info("Apple Silicon GPU acceleration enabled")
        
    elif self.platform in ["NVIDIA CUDA", "AMD ROCm"] and self.config.use_gpu:
        self.device = torch.device('cuda')
        self.gpu_available = True
        logger.info(f"{self.platform} GPU acceleration enabled")
        
    elif self.platform == "AMD DirectML" and self.config.use_gpu:
        import torch_directml
        self.device = torch_directml.device()
        self.gpu_available = True
        logger.info("AMD DirectML GPU acceleration enabled")
        
    else:
        self.device = torch.device('cpu')
        self.gpu_available = False
        logger.info("Using CPU for computations")
```

## ğŸ‰ æ€»ç»“

**âœ… Apple ARMèŠ¯ç‰‡**: å®Œå…¨æ”¯æŒï¼Œæ€§èƒ½ä¼˜ç§€ï¼Œæ¨èä½¿ç”¨
**âš ï¸ AMDæ˜¾å¡**: æ”¯æŒä½†éœ€è¦é¢å¤–é…ç½®ï¼ŒLinuxç¯å¢ƒä¸‹æ•ˆæœæœ€ä½³

æˆ‘ä»¬çš„GPUåŠ é€Ÿå®ç°å·²ç»ä¸ºå¤šå¹³å°åšå¥½äº†å‡†å¤‡ï¼Œåªéœ€è¦æ ¹æ®æ‚¨çš„ç¡¬ä»¶å¹³å°å®‰è£…ç›¸åº”çš„ä¾èµ–å³å¯äº«å—GPUåŠ é€Ÿå¸¦æ¥çš„æ€§èƒ½æå‡ï¼
